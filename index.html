<table style="width:100%; border-collapse: collapse;" border="1">
  <thead style="background-color:#f2f2f2;">
    <tr>
      <th style="padding: 10px; text-align: left;">Paper</th>
      <th style="padding: 10px; text-align: left;">Objectives</th>
      <th style="padding: 10px; text-align: left;">Methodology</th>
      <th style="padding: 10px; text-align: left;">Proposed Solution</th>
      <th style="padding: 10px; text-align: left;">Technology</th>
      <th style="padding: 10px; text-align: left;">Gaps</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      [cite_start]<td style="padding: 8px; vertical-align: top;"><b>Lost and Found Platform</b><br>[cite: 6, 433]</td>
      [cite_start]<td style="padding: 8px; vertical-align: top;">Overcome the inefficiencies of traditional, manual lost-and-found systems by creating a fast, reliable, and accessible digital platform for reporting and searching for items[cite: 14, 27, 441, 442, 4454].</td>
      [cite_start]<td style="padding: 8px; vertical-align: top;">The system is built on a standard three-tier web architecture[cite: 44, 49, 53, 4470]. [cite_start]The user workflow includes registration, item reporting with images, manual searching of the database, and secure communication[cite: 58, 59, 60, 61, 4485, 4486, 4487, 4488].</td>
      [cite_start]<td style="padding: 8px; vertical-align: top;">A web platform for users to report and search for lost or found items, featuring user authentication, image uploads, real-time notifications, and a text-based search function[cite: 17, 18, 26, 4444, 4445, 4453].</td>
      [cite_start]<td style="padding: 8px; vertical-align: top;"><b>Frontend:</b> React.js [cite: 16, 44, 4443, 4471][cite_start]<br><b>Backend:</b> Node.js with Express.js [cite: 49, 4476][cite_start]<br><b>Database:</b> MongoDB [cite: 12, 16, 53, 4439, 4443, 4480][cite_start]<br><b>Authentication:</b> JWT [cite: 51, 4478]</td>
      [cite_start]<td style="padding: 8px; vertical-align: top;">The system relies on manual user searching and lacks any AI-powered image recognition for automated matching[cite: 37, 75, 41, 4464, 4468, 4502, 4539, 4541]. [cite_start]The authors identify future needs for better user verification, spam filtering, and overall scalability[cite: 68, 4495].</td>
    </tr>
    <tr>
      [cite_start]<td style="padding: 8px; vertical-align: top;"><b>ILIAS: Instance-Level Image retrieval At Scale</b><br>[cite: 2063]</td>
      [cite_start]<td style="padding: 8px; vertical-align: top;">To create a large-scale, multi-domain benchmark dataset (ILIAS) for instance-level image retrieval, addressing the shortcomings of previous datasets like small scale, lack of diversity, and ground-truth errors[cite: 2068, 2104, 2106].</td>
      [cite_start]<td style="padding: 8px; vertical-align: top;">Manually collected images for 1,000 diverse object instances[cite: 2071, 2111]. [cite_start]Used 100 million distractor images from YFCC100M (pre-2014) and only included query objects created *after* 2014 to ensure no false negatives in the ground truth[cite: 2072, 2073, 2112].</td>
      [cite_start]<td style="padding: 8px; vertical-align: top;">The **ILIAS dataset**, a new benchmark for instance-level image retrieval supporting both image-to-image and text-to-image queries[cite: 2068, 2114]. [cite_start]A smaller 'mini-ILIAS' version is also provided for faster experiments[cite: 2116, 2284].</td>
      [cite_start]<td style="padding: 8px; vertical-align: top;">The paper benchmarks various foundation models like **OAI-CLIP, OpenCLIP, and SigLIP**[cite: 2088, 2089, 2087]. [cite_start]It highlights the importance of re-ranking with **local descriptors** using methods like **AMES** for top performance[cite: 2099, 2323, 2375].</td>
      [cite_start]<td style="padding: 8px; vertical-align: top;">Current models struggle with the dataset's domain diversity and challenging conditions[cite: 2391]. [cite_start]There is a large performance gap between global representations and more accurate (but computationally expensive) local descriptor methods[cite: 2125, 2884].</td>
    </tr>
    <tr>
      [cite_start]<td style="padding: 8px; vertical-align: top;"><b>Localizing Objects with Self-Supervised Transformers and no Labels (LOST)</b><br>[cite: 4609]</td>
      [cite_start]<td style="padding: 8px; vertical-align: top;">To localize objects in images without any supervision or labels, using a scalable method that operates on single images rather than entire collections[cite: 4621, 4623, 4645].</td>
      [cite_start]<td style="padding: 8px; vertical-align: top;">Uses features from a **DINO** self-supervised **Vision Transformer (ViT)**[cite: 4622, 4646]. [cite_start]It identifies a "seed" patch within an object based on its low correlation with other patches in the image, then expands from this seed to create a bounding box[cite: 4649, 4651, 4652].</td>
      [cite_start]<td style="padding: 8px; vertical-align: top;">**LOST**, a simple, unsupervised method for single-object discovery[cite: 4623]. [cite_start]Also, a pipeline to train an object detector (**Faster R-CNN**) using LOST's outputs as pseudo-labels without any human annotation[cite: 4624, 4759].</td>
      [cite_start]<td style="padding: 8px; vertical-align: top;"><b>Vision Transformer (ViT)** with **DINO** pre-training[cite: 4622, 4778]. [cite_start]<br><b>Faster R-CNN** [cite: 4817] [cite_start]<br><b>K-means clustering** [cite: 4656, 4767]</td>
      [cite_start]<td style="padding: 8px; vertical-align: top;">The method is not designed to separate overlapping instances of the same object class[cite: 4854]. [cite_start]It may also fail if an object covers most of the image, as this violates a core assumption of the seed selection process[cite: 4857, 4858].</td>
    </tr>
    <tr>
      [cite_start]<td style="padding: 8px; vertical-align: top;"><b>LostNet: A smart way for lost and find</b><br>[cite: 1629]</td>
      [cite_start]<td style="padding: 8px; vertical-align: top;">To design an intelligent and lightweight system to simplify lost and found searches by automatically matching images of lost and recovered items[cite: 1640, 1647].</td>
      [cite_start]<td style="padding: 8px; vertical-align: top;">A two-stage system: first, it classifies an uploaded image into one of 10 categories using a modified MobileNetV2 [cite: 1981][cite_start]; second, it uses a **Perceptual Hash Algorithm** to find the most similar images within that category by comparing "fingerprint" strings[cite: 1983].</td>
      [cite_start]<td style="padding: 8px; vertical-align: top;">**LostNet**, a system combining a **MobileNetV2+CBAM** classifier for initial filtering with a perceptual hash algorithm for the final image similarity matching[cite: 1641, 1673, 1999].</td>
      [cite_start]<td style="padding: 8px; vertical-align: top;"><b>MobileNetV2** with Transfer Learning[cite: 1641, 1673]. [cite_start]<br><b>CBAM** attention module[cite: 1641, 1761, 2001]. [cite_start]<br><b>Perceptual Hash Algorithm**[cite: 1673, 1805, 1999]. [cite_start]<br><b>Spring Boot** backend[cite: 1978].</td>
      [cite_start]<td style="padding: 8px; vertical-align: top;">The system is strictly limited to 10 predefined categories[cite: 1674, 1815]. [cite_start]Its reliance on perceptual hashing is less capable of capturing semantic similarity compared to modern vector-based search methods[cite: 2008].</td>
    </tr>
    <tr>
      [cite_start]<td style="padding: 8px; vertical-align: top;"><b>Instance-level Image Retrieval using Reranking Transformers (RRTs)</b><br>[cite: 3763]</td>
      [cite_start]<td style="padding: 8px; vertical-align: top;">To improve the reranking stage of image retrieval by replacing expensive methods like geometric verification with a lightweight, learnable model that uses both global and local features[cite: 3769, 3770, 3771].</td>
      [cite_start]<td style="padding: 8px; vertical-align: top;">A **transformer architecture** is trained to predict an image pair's similarity score directly from their global and local feature descriptors[cite: 3771, 3808]. [cite_start]The model is trained on pairs of images with a binary label indicating if they contain the same instance[cite: 3895, 3898].</td>
      [cite_start]<td style="padding: 8px; vertical-align: top;">**Reranking Transformers (RRTs)**, a lightweight and efficient model designed as a drop-in replacement for traditional reranking techniques like geometric verification[cite: 3771, 3809]. [cite_start]It can also be jointly optimized with the feature extractor[cite: 3774].</td>
      [cite_start]<td style="padding: 8px; vertical-align: top;"><b>Transformer architecture**[cite: 3811]. [cite_start]<br>Evaluated using **DELG (ResNet50)** features[cite: 3920]. [cite_start]<br>Compared against **Geometric Verification (GV)** and **ASMK**[cite: 3802, 3803].</td>
      [cite_start]<td style="padding: 8px; vertical-align: top;">The model's single similarity score is less interpretable than the geometric alignment provided by GV[cite: 4174]. [cite_start]Like other learning-based methods, it may struggle with large domain shifts between training and test data[cite: 4181].</td>
    </tr>
    <tr>
      [cite_start]<td style="padding: 8px; vertical-align: top;"><b>Deep Learning for Instance Retrieval: A Survey</b><br>[cite: 182]</td>
      [cite_start]<td style="padding: 8px; vertical-align: top;">To survey and categorize the landscape of deep learning-based methods for instance-level image retrieval (IIR)[cite: 188, 221].</td>
      [cite_start]<td style="padding: 8px; vertical-align: top;">A comprehensive literature review that analyzes the field by breaking it down into key stages: deep feature extraction, feature embedding/aggregation, and network fine-tuning strategies[cite: 188, 222].</td>
      [cite_start]<td style="padding: 8px; vertical-align: top;"><b>Key Themes:</b> The paper identifies two main paradigms: using **off-the-shelf** pre-trained models and using **fine-tuned** models[cite: 224, 339]. It outlines a general IIR framework based on these approaches.</td>
      [cite_start]<td style="padding: 8px; vertical-align: top;">Reviews a broad spectrum of technologies, including DCNNs (**AlexNet, VGG**), feature aggregation methods (**R-MAC, CroW, VLAD**), and the emergence of **Transformers**[cite: 218, 232, 241, 247, 253].</td>
      [cite_start]<td style="padding: 8px; vertical-align: top;"><b>Identified Field Gaps:</b> Challenges remain in achieving **accuracy** (e.g., invariance to viewpoint, handling clutter) and **efficiency** (e.g., high-dimensional features, storage costs)[cite: 256, 344]. [cite_start]The field needs better few-shot and continual learning methods[cite: 1286, 1289].</td>
    </tr>
  </tbody>
</table>